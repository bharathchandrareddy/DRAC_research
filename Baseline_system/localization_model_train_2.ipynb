{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VDswVC6fTCDr","outputId":"78c679a0-3099-4e71-aec5-0fdd3d803631"},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9095,"status":"ok","timestamp":1714616133670,"user":{"displayName":"Yugandar Narra","userId":"14916928621674904417"},"user_tz":300},"id":"C6LXbPW3FbYA","outputId":"19c1e6fd-b6d0-4fb5-e2c3-967fd07a7d2a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: chainer in c:\\users\\pc\\anaconda3\\envs\\drac_baseline\\lib\\site-packages (7.8.1)\n","Requirement already satisfied: setuptools in c:\\users\\pc\\anaconda3\\envs\\drac_baseline\\lib\\site-packages (from chainer) (69.5.1)\n","Requirement already satisfied: typing-extensions in c:\\users\\pc\\appdata\\roaming\\python\\python39\\site-packages (from chainer) (4.12.2)\n","Requirement already satisfied: filelock in c:\\users\\pc\\anaconda3\\envs\\drac_baseline\\lib\\site-packages (from chainer) (3.13.1)\n","Requirement already satisfied: numpy>=1.9.0 in c:\\users\\pc\\anaconda3\\envs\\drac_baseline\\lib\\site-packages (from chainer) (1.25.0)\n","Requirement already satisfied: protobuf>=3.0.0 in c:\\users\\pc\\anaconda3\\envs\\drac_baseline\\lib\\site-packages (from chainer) (4.25.3)\n","Requirement already satisfied: six>=1.9.0 in c:\\users\\pc\\appdata\\roaming\\python\\python39\\site-packages (from chainer) (1.16.0)\n"]}],"source":["!pip install chainer"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: numpy==1.25 in c:\\users\\pc\\anaconda3\\envs\\drac_baseline\\lib\\site-packages (1.25.0)Note: you may need to restart the kernel to use updated packages.\n","\n"]}],"source":["pip install numpy==1.25"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: pillow in c:\\users\\pc\\anaconda3\\envs\\drac_baseline\\lib\\site-packages (10.3.0)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install pillow"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"qvZm6pcNFSMQ"},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\PC\\anaconda3\\envs\\drac_baseline\\lib\\site-packages\\chainer\\_environment_check.py:72: UserWarning: \n","--------------------------------------------------------------------------------\n","CuPy (cupy) version 13.2.0 may not be compatible with this version of Chainer.\n","Please consider installing the supported version by running:\n","  $ pip install 'cupy>=7.7.0,<8.0.0'\n","\n","See the following page for more details:\n","  https://docs.cupy.dev/en/latest/install.html\n","--------------------------------------------------------------------------------\n","\n","  warnings.warn(msg.format(\n"]}],"source":["import os\n","import numpy as np\n","import random\n","\n","\n","import argparse\n","import numpy as np\n","\n","import chainer\n","import chainer.functions as F\n","import chainer.links as L\n","\n","import chainer\n","import chainer.functions as F\n","import chainer.links as L\n","from chainer import training,serializers\n","from chainer.training import extensions\n","from PIL import Image"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"yzgWALIET3B8"},"outputs":[],"source":["# Define location of the images and labels\n","xbd_dir=\"C:\\\\Users\\\\PC\\\\Desktop\\\\damage_assessement_data\\\\train\"\n","images_dir=os.path.join(xbd_dir,\"images\")\n","labels_dir=os.path.join(xbd_dir,\"labels\")\n","masks_dir = os.path.join(xbd_dir, 'masks')\n","extracted_files = \"C:\\\\Users\\\\PC\\\\Desktop\\\\drac_saved_files\"\n","train_data_info=os.path.join(extracted_files,\"train_data_info.csv\")\n","\n","train_batch_size = 2\n","test_batch_size = 2\n","epochs = 100\n","frequency = 25\n","log_dir = os.path.join(extracted_files,\"logs\")"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"RZBlc67ZFSMQ"},"outputs":[],"source":["# Read mask and convert to array\n","def read_mask_image(path, dtype):\n","    with Image.open(path) as img:\n","        #convert image to binary format\n","        img = img.convert('1')\n","        image = np.array(img, dtype=dtype)\n","    return image"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"okbudUVhFSMQ"},"outputs":[],"source":["# Read image and convert to array\n","def read_disaster_image(path, dtype):\n","    with Image.open(path) as img:\n","        image = np.array(img, dtype=dtype)\n","    return image"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"wqkV8xkxFSMR"},"outputs":[],"source":["from chainer.dataset import dataset_mixin\n","class CustomDataset(dataset_mixin.DatasetMixin):\n","    def __init__(self, train_test_data_path, images_path, masks_path, image_dtype=np.float32, mask_dtype=np.int32, mean=0, crop_size=400, is_test=False):\n","        with open(train_test_data_path) as file:\n","            image_mask_pairs = []\n","            for line in file:\n","                line = line.rstrip('\\n')\n","                image_filename = label_filename = line\n","                image_mask_pairs.append((image_filename, label_filename))\n","        self.image_mask_pairs = image_mask_pairs\n","        self.images_path = images_path\n","        self.masks_path = masks_path\n","        self.image_dtype = image_dtype\n","        self.mask_dtype = mask_dtype\n","        self.mean = mean[np.newaxis, np.newaxis, :]\n","        self.crop_size = crop_size\n","        self.is_test = is_test\n","\n","    def __len__(self):\n","        return len(self.image_mask_pairs)\n","\n","    def get_example(self, index):\n","        image_filename, mask_filename = self.image_mask_pairs[index]\n","        images_path = os.path.join(self.images_path, image_filename)\n","        image = read_disaster_image(images_path, self.image_dtype)\n","        image = (image - self.mean) / 255.0\n","        masks_path = os.path.join(self.masks_path, mask_filename)\n","        mask_image = read_mask_image(masks_path, self.mask_dtype)\n","        h, w, c = image.shape\n","        mask = np.zeros(shape=[h, w], dtype=np.int32)\n","        mask[mask_image > 0] = 1\n","        #compare the size of image to desired crop size\n","        #If image size is less than crop size add padding to ensure size requirement\n","        if (h < self.crop_size) or (w < self.crop_size):\n","            H, W = max(h, self.crop_size), max(w, self.crop_size)\n","            padding_x1, padding_y1 = (W - w) // 2, (H - h) // 2\n","            padding_x2, padding_y2 = (W - w - padding_x1), (H - h - padding_y1)\n","            image = np.pad(image, ((padding_y1, padding_y2), (padding_x1, padding_x2), (0, 0)), 'symmetric')\n","            if self.is_test:\n","                mask = np.pad(mask, ((padding_y1, padding_y2), (padding_x1, padding_x2)), 'constant', constant_values=255)\n","            else:\n","                mask = np.pad(mask, ((padding_y1, padding_y2), (padding_x1, padding_x2)), 'symmetric')\n","            h, w = H, W\n","            if not self.is_test:\n","                # Randomly flip horizontally\n","                if random.choice([True, False]):\n","                    image = image[:, ::-1, :]\n","                    mask = mask[:, ::-1]\n","                # Randomly flip vertically\n","                if random.choice([True, False]):\n","                    image = image[::-1, :, :]\n","                    mask = mask[::-1, :]\n","                # Randomly crop\n","                top = random.randint(0, h - self.crop_size)\n","                left = random.randint(0, w - self.crop_size)\n","            else:\n","                top = (h - self.crop_size) // 2\n","                left = (w - self.crop_size) // 2\n","            bottom = top + self.crop_size\n","            right = left + self.crop_size\n","            image = image[top:bottom, left:right]\n","            mask = mask[top:bottom, left:right]\n","            print(image.transpose(2, 0, 1), mask)\n","        return image.transpose(2, 0, 1), mask\n"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"3yjTROZLSwg3"},"outputs":[],"source":["import chainer\n","import chainer.functions as F\n","import chainer.links as L\n","\n","\n","class UNet(chainer.Chain):\n","\n","    def __init__(self):\n","        super(UNet, self).__init__(\n","            c0=L.Convolution2D(3, 32, 3, 1, 1),\n","            c1=L.Convolution2D(32, 64, 4, 2, 1),\n","            c2=L.Convolution2D(64, 64, 3, 1, 1),\n","            c3=L.Convolution2D(64, 128, 4, 2, 1),\n","            c4=L.Convolution2D(128, 128, 3, 1, 1),\n","            c5=L.Convolution2D(128, 256, 4, 2, 1),\n","            c6=L.Convolution2D(256, 256, 3, 1, 1),\n","            c7=L.Convolution2D(256, 512, 4, 2, 1),\n","            c8=L.Convolution2D(512, 512, 3, 1, 1),\n","\n","            dc8=L.Deconvolution2D(1024, 512, 4, 2, 1),\n","            dc7=L.Convolution2D(512, 256, 3, 1, 1),\n","            dc6=L.Deconvolution2D(512, 256, 4, 2, 1),\n","            dc5=L.Convolution2D(256, 128, 3, 1, 1),\n","            dc4=L.Deconvolution2D(256, 128, 4, 2, 1),\n","            dc3=L.Convolution2D(128, 64, 3, 1, 1),\n","            dc2=L.Deconvolution2D(128, 64, 4, 2, 1),\n","            dc1=L.Convolution2D(64, 32, 3, 1, 1),\n","            dc0=L.Convolution2D(64, 2, 3, 1, 1),\n","            bnc0=L.BatchNormalization(32),\n","            bnc1=L.BatchNormalization(64),\n","            bnc2=L.BatchNormalization(64),\n","            bnc3=L.BatchNormalization(128),\n","            bnc4=L.BatchNormalization(128),\n","            bnc5=L.BatchNormalization(256),\n","            bnc6=L.BatchNormalization(256),\n","            bnc7=L.BatchNormalization(512),\n","            bnc8=L.BatchNormalization(512),\n","            bnd8=L.BatchNormalization(512),\n","            bnd7=L.BatchNormalization(256),\n","            bnd6=L.BatchNormalization(256),\n","            bnd5=L.BatchNormalization(128),\n","            bnd4=L.BatchNormalization(128),\n","            bnd3=L.BatchNormalization(64),\n","            bnd2=L.BatchNormalization(64),\n","            bnd1=L.BatchNormalization(32)\n","        )\n","\n","\n","    def forward(self, x):\n","        x = x.astype(np.float32)\n","        e0 = F.relu(self.bnc0(self.c0(x)))\n","        e1 = F.relu(self.bnc1(self.c1(e0)))\n","        e2 = F.relu(self.bnc2(self.c2(e1)))\n","        del e1\n","        e3 = F.relu(self.bnc3(self.c3(e2)))\n","        e4 = F.relu(self.bnc4(self.c4(e3)))\n","        del e3\n","        e5 = F.relu(self.bnc5(self.c5(e4)))\n","        e6 = F.relu(self.bnc6(self.c6(e5)))\n","        del e5\n","        e7 = F.relu(self.bnc7(self.c7(e6)))\n","        e8 = F.relu(self.bnc8(self.c8(e7)))\n","\n","        d8 = F.relu(self.bnd8(self.dc8(F.concat([e7, e8]))))\n","        del e7, e8\n","        d7 = F.relu(self.bnd7(self.dc7(d8)))\n","        del d8\n","        d6 = F.relu(self.bnd6(self.dc6(F.concat([e6, d7]))))\n","        del d7, e6\n","        d5 = F.relu(self.bnd5(self.dc5(d6)))\n","        del d6\n","        d4 = F.relu(self.bnd4(self.dc4(F.concat([e4, d5]))))\n","        del d5, e4\n","        d3 = F.relu(self.bnd3(self.dc3(d4)))\n","        del d4\n","        d2 = F.relu(self.bnd2(self.dc2(F.concat([e2, d3]))))\n","        del d3, e2\n","        d1 = F.relu(self.bnd1(self.dc1(d2)))\n","        del d2\n","        d0 = self.dc0(F.concat([e0, d1]))\n","        return d0\n","\n","\n","    def __call__(self, x, t):\n","        h = self.forward(x)\n","        loss = F.softmax_cross_entropy(h, t, ignore_label=255)\n","        accuracy = F.accuracy(h, t, ignore_label=255)\n","        chainer.report({'loss': loss, 'accuracy': accuracy}, self)\n","        return loss"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":835,"status":"ok","timestamp":1714616326602,"user":{"displayName":"Yugandar Narra","userId":"14916928621674904417"},"user_tz":300},"id":"9zs1ir4KFSMR","outputId":"52faf56f-f03a-4896-a403-93df92fd7295"},"outputs":[{"name":"stdout","output_type":"stream","text":["UNet(\n","  (bnc0): BatchNormalization(size=32, decay=0.9, eps=2e-05, dtype=float32, use_gamma=True, use_beta=True),\n","  (bnc1): BatchNormalization(size=64, decay=0.9, eps=2e-05, dtype=float32, use_gamma=True, use_beta=True),\n","  (bnc2): BatchNormalization(size=64, decay=0.9, eps=2e-05, dtype=float32, use_gamma=True, use_beta=True),\n","  (bnc3): BatchNormalization(size=128, decay=0.9, eps=2e-05, dtype=float32, use_gamma=True, use_beta=True),\n","  (bnc4): BatchNormalization(size=128, decay=0.9, eps=2e-05, dtype=float32, use_gamma=True, use_beta=True),\n","  (bnc5): BatchNormalization(size=256, decay=0.9, eps=2e-05, dtype=float32, use_gamma=True, use_beta=True),\n","  (bnc6): BatchNormalization(size=256, decay=0.9, eps=2e-05, dtype=float32, use_gamma=True, use_beta=True),\n","  (bnc7): BatchNormalization(size=512, decay=0.9, eps=2e-05, dtype=float32, use_gamma=True, use_beta=True),\n","  (bnc8): BatchNormalization(size=512, decay=0.9, eps=2e-05, dtype=float32, use_gamma=True, use_beta=True),\n","  (bnd1): BatchNormalization(size=32, decay=0.9, eps=2e-05, dtype=float32, use_gamma=True, use_beta=True),\n","  (bnd2): BatchNormalization(size=64, decay=0.9, eps=2e-05, dtype=float32, use_gamma=True, use_beta=True),\n","  (bnd3): BatchNormalization(size=64, decay=0.9, eps=2e-05, dtype=float32, use_gamma=True, use_beta=True),\n","  (bnd4): BatchNormalization(size=128, decay=0.9, eps=2e-05, dtype=float32, use_gamma=True, use_beta=True),\n","  (bnd5): BatchNormalization(size=128, decay=0.9, eps=2e-05, dtype=float32, use_gamma=True, use_beta=True),\n","  (bnd6): BatchNormalization(size=256, decay=0.9, eps=2e-05, dtype=float32, use_gamma=True, use_beta=True),\n","  (bnd7): BatchNormalization(size=256, decay=0.9, eps=2e-05, dtype=float32, use_gamma=True, use_beta=True),\n","  (bnd8): BatchNormalization(size=512, decay=0.9, eps=2e-05, dtype=float32, use_gamma=True, use_beta=True),\n","  (c0): Convolution2D(in_channels=3, out_channels=32, ksize=3, stride=(1, 1), pad=(1, 1), nobias=False, dilate=(1, 1), groups=1),\n","  (c1): Convolution2D(in_channels=32, out_channels=64, ksize=4, stride=(2, 2), pad=(1, 1), nobias=False, dilate=(1, 1), groups=1),\n","  (c2): Convolution2D(in_channels=64, out_channels=64, ksize=3, stride=(1, 1), pad=(1, 1), nobias=False, dilate=(1, 1), groups=1),\n","  (c3): Convolution2D(in_channels=64, out_channels=128, ksize=4, stride=(2, 2), pad=(1, 1), nobias=False, dilate=(1, 1), groups=1),\n","  (c4): Convolution2D(in_channels=128, out_channels=128, ksize=3, stride=(1, 1), pad=(1, 1), nobias=False, dilate=(1, 1), groups=1),\n","  (c5): Convolution2D(in_channels=128, out_channels=256, ksize=4, stride=(2, 2), pad=(1, 1), nobias=False, dilate=(1, 1), groups=1),\n","  (c6): Convolution2D(in_channels=256, out_channels=256, ksize=3, stride=(1, 1), pad=(1, 1), nobias=False, dilate=(1, 1), groups=1),\n","  (c7): Convolution2D(in_channels=256, out_channels=512, ksize=4, stride=(2, 2), pad=(1, 1), nobias=False, dilate=(1, 1), groups=1),\n","  (c8): Convolution2D(in_channels=512, out_channels=512, ksize=3, stride=(1, 1), pad=(1, 1), nobias=False, dilate=(1, 1), groups=1),\n","  (dc0): Convolution2D(in_channels=64, out_channels=2, ksize=3, stride=(1, 1), pad=(1, 1), nobias=False, dilate=(1, 1), groups=1),\n","  (dc1): Convolution2D(in_channels=64, out_channels=32, ksize=3, stride=(1, 1), pad=(1, 1), nobias=False, dilate=(1, 1), groups=1),\n","  (dc2): Deconvolution2D(),\n","  (dc3): Convolution2D(in_channels=128, out_channels=64, ksize=3, stride=(1, 1), pad=(1, 1), nobias=False, dilate=(1, 1), groups=1),\n","  (dc4): Deconvolution2D(),\n","  (dc5): Convolution2D(in_channels=256, out_channels=128, ksize=3, stride=(1, 1), pad=(1, 1), nobias=False, dilate=(1, 1), groups=1),\n","  (dc6): Deconvolution2D(),\n","  (dc7): Convolution2D(in_channels=512, out_channels=256, ksize=3, stride=(1, 1), pad=(1, 1), nobias=False, dilate=(1, 1), groups=1),\n","  (dc8): Deconvolution2D(),\n",")\n"]}],"source":["# Initialize UNet()\n","model = UNet()\n","\n","chainer.cuda.get_device_from_id(0).use()\n","model = model.to_gpu()\n","print(model)"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"MrhJKR7AFXfB"},"outputs":[{"data":{"text/plain":["<chainer.optimizers.adam.Adam at 0x16ba258eb20>"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["# Setup Adam optimizer\n","optimizer = chainer.optimizers.Adam()\n","optimizer.setup(model)"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"f6PB5lJpFSMR"},"outputs":[],"source":["# Load Train and Test Datasets\n","mean = np.load(os.path.join(extracted_files, \"mean.npy\"))\n","\n","train_dataset = CustomDataset(os.path.join(extracted_files, \"train.txt\"), images_dir, masks_dir, mean=mean, crop_size=400, is_test=False)\n","\n","test_dataset = CustomDataset (os.path.join(extracted_files, \"test.txt\"), images_dir, masks_dir, mean=mean, crop_size=480, is_test=True)\n","\n","train_iterator = chainer.iterators.SerialIterator(train_dataset, train_batch_size,shuffle=True)\n","test_iterator = chainer.iterators.SerialIterator(test_dataset, test_batch_size, shuffle=False)"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"Ftxsw6SmFSMR"},"outputs":[],"source":["# Set up a trainer\n","updater = training.StandardUpdater(train_iterator, optimizer, device=0)\n","trainer = training.Trainer(updater, (epochs, 'epoch'), out=log_dir)"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"dmhLh785FSMR"},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\PC\\anaconda3\\envs\\drac_baseline\\lib\\site-packages\\chainer\\training\\extensions\\evaluator.py:130: UserWarning: The `repeat` property of the iterator {} \n","  warnings.warn(msg)\n"]}],"source":["# Evaluate the model with the test dataset\n","trainer.extend(extensions.Evaluator(test_iterator, model, device=0))"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"PfAFF7KXFSMR"},"outputs":[],"source":["# logging\n","trainer.extend(extensions.LogReport())\n","\n","# Print report for each epoch\n","trainer.extend(extensions.PrintReport(['epoch', 'main/loss', 'validation/main/loss','main/accuracy', 'validation/main/accuracy', 'elapsed_time']))\n","\n","# Print a progress bar\n","trainer.extend(extensions.ProgressBar())"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Is CUDA available? True\n","CUDA device count: 1\n","CUDA device name: NVIDIA GeForce RTX 3060\n"]}],"source":["import torch\n","\n","print(\"Is CUDA available?\", torch.cuda.is_available())\n","print(\"CUDA device count:\", torch.cuda.device_count())\n","print(\"CUDA device name:\", torch.cuda.get_device_name(0))"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Allocated: 0.00 GB, Reserved: 0.00 GB\n"]}],"source":["import torch\n","\n","# Function to monitor GPU memory usage\n","def print_gpu_memory():\n","    allocated = torch.cuda.memory_allocated() / 1024**3\n","    reserved = torch.cuda.memory_reserved() / 1024**3\n","    print(f\"Allocated: {allocated:.2f} GB, Reserved: {reserved:.2f} GB\")\n","\n","print_gpu_memory()\n"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SinlBr3TS80y","outputId":"259d3db9-962c-4de0-932a-0a1b64b58ce4"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n","\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n","\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n","\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["# Run the trainer\n","trainer.run()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dqauVy8RrUlJ"},"outputs":[],"source":["model_save_path = 'C:\\\\Users\\\\PC\\\\Desktop\\\\drac_saved_files\\\\localization_model'\n","chainer.serializers.save_npz(model_save_path, model)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"drac_baseline","language":"python","name":"drac_baseline"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.19"}},"nbformat":4,"nbformat_minor":0}
